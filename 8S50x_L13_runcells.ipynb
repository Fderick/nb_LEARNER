{"cells": [{"cell_type": "markdown", "id": "1a7c0d85", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 13: Deep Learning</h1>\n"]}, {"cell_type": "markdown", "id": "6a6864bf", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_13_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L13.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "291c0c79", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_13_1\">L13.1 Machine Learning</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_13_1\">L13.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_13_2\">L13.2 Procedure and Example Algorithms</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_13_2\">L13.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_13_3\">L13.3 Logistic Regression Algorithm</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_13_3\">L13.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_13_4\">L13.4 Introduction to Neural Networks</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_13_4\">L13.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_13_5\">L13.5 What Do Neural Networks Do?</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_13_5\">L13.5 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_13_6\">L13.6 Training a Neural Network</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_13_6\">L13.6 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_13_7\">L13.7 Training a Neural Network with PyTorch</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_13_7\">L13.7 Exercises</a></td>\n", "    </tr>\n", "</table>"]}, {"cell_type": "code", "execution_count": null, "id": "0b796a1c", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.0-runcell00\n", "\n", "!git init\n", "!git remote add -f origin https://github.com/mitx-8s50/nb_LEARNER/\n", "!git config core.sparseCheckout true\n", "!echo 'L13' >> .git/info/sparse-checkout\n", "!git pull origin main"]}, {"cell_type": "code", "execution_count": null, "id": "dbb547d9", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.0-runcell01\n", "\n", "#do the following if running this notebook locally (if needed)\n", "#within your conda environment, run the following\n", "#conda install pytorch"]}, {"cell_type": "code", "execution_count": null, "id": "c517e659", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.0-runcell02\n", "\n", "import torch                      #https://pytorch.org/\n", "import numpy as np                #https://numpy.org/doc/stable/ \n", "import matplotlib.pyplot as plt   #https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html\n", "from scipy import stats           #https://docs.scipy.org/doc/scipy/reference/stats.html\n", "import scipy.optimize as opt      #https://docs.scipy.org/doc/scipy/reference/optimize.html"]}, {"cell_type": "code", "execution_count": null, "id": "a3785d45", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.0-runcell03\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title\n"]}, {"cell_type": "markdown", "id": "da500ade", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_13_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L13.1 Machine Learning</h2>  \n", "\n", "| [Top](#section_13_0) | [Previous Section](#section_13_0) | [Exercises](#exercises_13_1) | [Next Section](#section_13_2) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "45f162e7", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.1-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L13/slides_L13_01.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "35eae3b6", "metadata": {"tags": ["learner", "py", "lect_01", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.1-runcell01\n", "\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "#NOTE: this sets the random seed\n", "#the random data will look slightly different from the video\n", "np.random.seed(42)\n", "\n", "#define points\n", "x1 = np.random.normal(0, 5, 100) #mean, sigma, num_points\n", "x2 = np.random.normal(0, 5, 100)\n", "\n", "noise = 1. #sets the noise scale\n", "x1noise = np.random.normal(0, noise, 100)\n", "x2noise = np.random.normal(0, noise, 100)\n", "\n", "mask_pos = x2 > (2. - 3.*x1)\n", "mask_neg = x2 <= (2. - 3.*x1)\n", "\n", "x1 = x1 + x1noise\n", "x2 = x2 + x2noise\n", "\n", "# let's define a true boundary between the two classes\n", "# by x_2 = 2 - 3 x_1\n", "x1_pos = x1[mask_pos]\n", "x2_pos = x2[mask_pos]\n", "x1_neg = x1[mask_neg]\n", "x2_neg = x2[mask_neg]\n", "\n", "\n", "#look at things\n", "plt.plot(x1_pos, x2_pos, 'r+')\n", "plt.plot(x1_neg, x2_neg, 'b.')\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "a33ed572", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_13_1'></a>     \n", "\n", "| [Top](#section_13_0) | [Restart Section](#section_13_1) | [Next Section](#section_13_2) |\n"]}, {"cell_type": "markdown", "id": "0c43e5b3", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.1.1</span>\n", "\n", "Change the `noise` parameter in the code that you previously ran. What do you observe about the red and blue points as the `noise` value is *increased*? **Afterwards set the noise parameter back to its original value, `noise = 1.`, as other problems will depend on the original form.**\n", "\n", "Choose the best answer from the following:\n", "\n", "A) Increasing the noise adds vertical jitter to the points.\\\n", "B) Increasing the noise adds more points to the plot.\\\n", "C) Increasing the noise causes the points to mix across the line that defines the boundary between the two classes.\\\n", "D) Increasing the noise simply makes the random selection of point locations different from run to run.\\"]}, {"cell_type": "markdown", "id": "9f489916", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.1.2</span>\n", "\n", "Which of the following statements best describes the difference between machine learning and supervised learning?\n", "\n", "A) Machine learning is a type of learning where the algorithm learns from labeled data, while supervised learning is a broader term that includes different types of learning.\\\n", "B) Supervised learning is a type of learning where the algorithm learns from labeled data, while machine learning is a broader term that includes different types of learning.\\\n", "C) Machine learning and supervised learning are interchangeable terms that describe the same thing."]}, {"cell_type": "markdown", "id": "4b52e45c", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.1.3</span>\n", "\n", "Consider a data set with 10 classes. What would be the length of the one-hot-encoding vector that describes one of the classes? Enter your answer as an integer."]}, {"cell_type": "markdown", "id": "2c44db91", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_13_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L13.2 Procedure and Example Algorithms</h2>  \n", "\n", "| [Top](#section_13_0) | [Previous Section](#section_13_1) | [Exercises](#exercises_13_2) | [Next Section](#section_13_3) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "fb775721", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.2-runcell01\n", "\n", "def perceptron_loss(x,y,w,b):\n", "    loss = 0.\n", "    for i in range(len(x[0])):\n", "        fm = np.sign(x[0][i]*w[0]+x[1][i]*w[1]+b)\n", "        loss = loss + max(0.,-1.*y[i]*fm)\n", "    return loss\n", "\n", "x_list = [np.concatenate((x1_pos,x1_neg)),np.concatenate((x2_pos,x2_neg))]\n", "y_list = [1.]*len(x1_pos) + [-1.]*len(x1_neg)\n", "\n", "\n", "#NOTE: because our random data are different from the video,\n", "#these results will look slightly different from the video\n", "print(\"Perceptron loss (w1=3, w2=1, b=-2)\")\n", "print(perceptron_loss(x_list,y_list,[3.,1.],-2.))\n", "print()\n", "print(\"Perceptron loss (w1=3, w2=1, b=-10.)\")\n", "print(perceptron_loss(x_list,y_list,[3.,1.],-10.))\n", "print()\n", "print(\"Perceptron loss (w1=3.001, w2=1, b=-2)\")\n", "print(perceptron_loss(x_list,y_list,[3.,1.],-2.1))"]}, {"cell_type": "code", "execution_count": null, "id": "1a4d56a0", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.2-runcell02\n", "\n", "#create grid\n", "x1_list = np.linspace(-20., 20., 100)\n", "x2_list = np.linspace(-20., 20., 100)\n", "x1_grid, x2_grid  = np.meshgrid(x1_list, x2_list)\n", "\n", "#fill with model value\n", "h_grid = np.sign(3.*x1_grid + x2_grid - 2. )\n", "\n", "#draw 2d mesh\n", "plt.pcolormesh(x1_grid, x2_grid, h_grid, cmap = 'bwr', shading='auto')\n", "\n", "plt.plot(x1_pos, x2_pos, 'k+')\n", "plt.plot(x1_neg, x2_neg, 'k.')\n", "\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "f7ae62a2", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.2-runcell03\n", "\n", "def svm_loss(x,y,w,b):\n", "    loss = 0.\n", "    for i in range(len(x[0])):\n", "        fm = x[0][i]*w[0]+x[1][i]*w[1]+b\n", "        loss = loss + max(0.,1.-1.*y[i]*fm)\n", "    return loss\n", "\n", "#NOTE: because our random data are different from the video,\n", "#these results will look slightly different from the video\n", "print(\"SVM loss (w1=3, w2=1, b=-2)\")\n", "print(svm_loss(x_list,y_list,[3.,1.],-2.))\n", "print()\n", "print(\"SVM loss (w1=3, w2=1, b=-1.9)\")\n", "print(svm_loss(x_list,y_list,[3.,1.],-1.9))\n", "print()\n", "print(\"SVM loss (w1=3, w2=1, b=-2.1)\")\n", "print(svm_loss(x_list,y_list,[3.,1.],-2.1))"]}, {"cell_type": "code", "execution_count": null, "id": "cd59d801", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.2-runcell04\n", "\n", "#create grid\n", "x1_list = np.linspace(-20., 20., 100)\n", "x2_list = np.linspace(-20., 20., 100)\n", "x1_grid, x2_grid  = np.meshgrid(x1_list, x2_list)\n", "\n", "#fill with model value\n", "h_grid = np.sign(3.*x1_grid + x2_grid - 2. )\n", "\n", "\n", "#draw 2d mesh\n", "plt.pcolormesh(x1_grid, x2_grid, h_grid, cmap = 'bwr', shading='auto')\n", "\n", "plt.plot(x1_pos, x2_pos, 'k+')\n", "plt.plot(x1_neg, x2_neg, 'k.')\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "7423d891", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_13_2'></a>     \n", "\n", "| [Top](#section_13_0) | [Restart Section](#section_13_2) | [Next Section](#section_13_3) |\n"]}, {"cell_type": "markdown", "id": "29aa3bef", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.2.1</span>\n", "\n", "A loss function that returns zero indicates that the prediction is:\n", "\n", "A) correct\\\n", "B) incorrect"]}, {"cell_type": "markdown", "id": "b6709ce3", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.2.2</span>\n", "\n", "In the example in this section, the main difference between the perceptron loss function and the support vector machine loss function is that:\n", "\n", "A) The perceptron loss function is only suitable for linearly separable data, while the support vector machine loss function can handle non-linearly separable data.\\\n", "B) The support vector machine loss function only considers the samples that are closest to a boundary, while the perceptron loss function takes into account all samples.\\\n", "C) The perceptron loss function is a step function (correct vs. incorrect), whereas the support vector machine loss function assigns a continuous value to a data point, based on how close it is to a boundary."]}, {"cell_type": "markdown", "id": "8983a5e0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.2.3</span>\n", "\n", "Write code in the cell provided in the notebook to compute the perceptron loss for a slope of the dividing line within the range of 2 to 4 (while keeping the constant term at -2), and then vary the constant within the range of -2 to -4 (while keeping the slope at 3). Do you still see the same loss? What is the minimum?\n", "\n", "Specifically, varying which parameter will produce the GREATER variation in loss?\n", "    \n", "A) varying slope\\\n", "B) varying constant"]}, {"cell_type": "code", "execution_count": null, "id": "fcc3e872", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L13.2.3\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n"]}, {"cell_type": "markdown", "id": "7e4b0260", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.2.4</span>\n", "\n", "Let's say we have a single point in our distribution, at `(100,100)`. How does this change the svm loss if it is correct vs. incorrect?\n", "\n", "Calculate the svm loss for both cases, using the code cell in your notebook, and report your answer as a list of numbers with precision 1e-1: `[correct, incorrect]`"]}, {"cell_type": "code", "execution_count": null, "id": "9e0c241c", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L13.2.4\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n"]}, {"cell_type": "markdown", "id": "7b4d97d8", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_13_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L13.3 Logistic Regression Algorithm</h2>  \n", "\n", "| [Top](#section_13_0) | [Previous Section](#section_13_2) | [Exercises](#exercises_13_3) | [Next Section](#section_13_4) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "14b5b9a0", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.3-runcell01\n", "\n", "#switch to [0,1] from [-1,1]\n", "y_list_0 = [1.]*len(x1_pos) + [0.]*len(x1_neg)\n", "\n", "def lr_loss(x,y,w,b,scale=1.):\n", "    loss = 0.\n", "    for i in range(len(x[0])):\n", "        expon = -1.*scale*(x[0][i]*w[0]+x[1][i]*w[1]+b)\n", "        if (expon > 99.):\n", "            expon = 99.\n", "        fm = 1./(1.+np.exp(expon))\n", "        if (fm < 1. and fm > 0.):\n", "            loss = loss - y[i]*np.log(fm) - (1.-y[i])*np.log(1.-fm)\n", "    return loss\n", "\n", "\n", "#NOTE, the outputs in this notebook will be different from the video\n", "#because we have used a different random seed\n", "print('>>>Setting scale = 1')\n", "print(\"Logistic regression loss (w1=3, w2=1, b=-2) [scale = 1]\")\n", "print(lr_loss(x_list,y_list_0,[3.,1.],-2.))\n", "print()\n", "print(\"Logistic regression loss (w1=3, w2=1, b=-1.9) [scale = 1]\")\n", "print(lr_loss(x_list,y_list_0,[3.,1.],-1.9))\n", "print()\n", "print(\"Logistic regression loss (w1=3, w2=1, b=-2.1) [scale = 1]\")\n", "print(lr_loss(x_list,y_list_0,[3.,1.],-2.1))\n", "print()\n", "print('>>>Setting scale = 1.5')\n", "print(\"Logistic regression loss (w1=3, w2=1, b=-2) [scale = 1.5]\")\n", "print(lr_loss(x_list,y_list_0,[3.,1.],-2.,1.5))\n", "print()\n", "print('>>>Setting scale = 0.5')\n", "print(\"Logistic regression loss (w1=3, w2=1, b=-2) [scale = 0.5]\")\n", "print(lr_loss(x_list,y_list_0,[3.,1.],-2,0.5))"]}, {"cell_type": "code", "execution_count": null, "id": "ad25a791", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.3-runcell02\n", "\n", "#fill with model value - try changing the scale parameter\n", "h_grid = 1./(1.+np.exp(-1.*0.5*(x2_grid - (2. - 3.*x1_grid))))\n", "\n", "#draw 2d mesh\n", "plt.pcolormesh(x1_grid, x2_grid, h_grid, cmap = 'bwr', shading='auto')\n", "plt.colorbar()\n", "\n", "plt.plot(x1_pos, x2_pos, 'k+')\n", "plt.plot(x1_neg, x2_neg, 'k.')\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "b14d0f1f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_13_3'></a>     \n", "\n", "| [Top](#section_13_0) | [Restart Section](#section_13_3) | [Next Section](#section_13_4) |\n"]}, {"cell_type": "markdown", "id": "e591c07a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.3.1</span>\n", "\n", "Complete the code below to plot the logistic curve in one dimension: $f(x|scale)=\\dfrac{1}{1+e^{-\\mathrm{scale}*x}}$\n", "\n", "Afterwards, try varying the parameters of the plot."]}, {"cell_type": "code", "execution_count": null, "id": "941fee53", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L13.3.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def logistic_curve(x,scale):\n", "    return #YOUR CODE HERE\n", "\n", "# Define the x values\n", "x = np.linspace(-10, 10, 1000)\n", "\n", "# Define the y values\n", "scale = 0.5\n", "y = logistic_curve(x, scale)\n", "\n", "# Plot the function\n", "plt.plot(x, y)\n", "plt.xlabel('x')\n", "plt.ylabel('y')\n", "plt.title('Logistic Curve with Scale = {}'.format(scale))\n", "plt.grid(True)\n", "plt.show()\n"]}, {"cell_type": "markdown", "id": "0c1cf0d7", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.3.2</span>\n", "\n", "How do the features of the logistic curve change as you increase the scale parameter?\n", "\n", "A) The curve shifts to the left\\\n", "B) The curve shifts to the right\\\n", "C) The curve becomes steeper\\\n", "D) The curve becomes flatter"]}, {"cell_type": "markdown", "id": "e7f342da", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.3.3</span>\n", "\n", "Consider the plot produced by code cell `L13.3-runcell02`. That code had a fixed factor of 0.5 in the exponent, rather than an adjustable `scale` parameter. Write code in your notebook to reproduce the plot using modified values of the `scale` parameter, first to 100 larger, then to 100 times smaller. How does this impact the value at a position on the lower left corner of the plot (-20,-20)? Choose the best answer from the following:\n", "\n", "\n", "A) Modifying the scale parameter has no effect on the z-axis value at (-20,-20).\\\n", "B) Increasing the scale parameter leads to a higher value at (-20,-20).\\\n", "C) Increasing the scale parameter has no effect on the z-axis value at (-20,-20).\\\n", "D) Increasing the scale parameter leads to a lower value at (-20,-20).\\\n", "E) Decreasing the scale parameter leads to a higher value at (-20,-20).\\\n", "F) Decreasing the scale parameter has no effect on the z-axis value at (-20,-20).\\\n", "G) Decreasing the scale parameter leads to a lower value at (-20,-20).\n", "\n"]}, {"cell_type": "code", "execution_count": null, "id": "3ed27a2f", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L13.3.3\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n"]}, {"cell_type": "markdown", "id": "bddbad5a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_13_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L13.4 Introduction to Neural Networks</h2>  \n", "\n", "| [Top](#section_13_0) | [Previous Section](#section_13_3) | [Exercises](#exercises_13_4) | [Next Section](#section_13_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "005c8934", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.4-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L13/slides_L13_04.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "0100f6fc", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_13_4'></a>     \n", "\n", "| [Top](#section_13_0) | [Restart Section](#section_13_4) | [Next Section](#section_13_5) |\n"]}, {"cell_type": "markdown", "id": "6e863fc6", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.4.1</span>\n", "\n", "Having introduced the concept of activation functions, let's try a different activation function in our output node, for the data set we have been studying. Specifically, we will replace the logistic function (i.e., sigmoid) with a `tanh` activation function.\n", "\n", "Since the `tanh` function outputs values over the range [-1,1], it is not suitable to use for calculating the Binary Cross Entropy (since BCE requires probabilities between [0,1]). Therefore, let's calculate the loss using the Support Vector Machine algorithm. Let's also convert the binary data back to the range [-1,1] (for now).\n", "\n", "**Fill in the code below to calculate the loss for the parameters given, and enter your answer as a number with precision 1e-2.**\n", "\n", "Try varying the parameters to see where the loss is minimized, as we have done previously. Does this loss function give results that are similar to the Binary Cross Entropy with sigmoid activation (i.e., what we did in `L13.3-runcell01`)?"]}, {"cell_type": "code", "execution_count": null, "id": "c13f3673", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L13.4.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def tanh_loss(x, y, w, b, scale=1.):\n", "    loss = 0.\n", "    for i in range(len(x[0])):\n", "        fm = #YOUR CODE HERE\n", "        loss = loss + (1-1.*y[i]*fm)        \n", "    return loss\n", "\n", "#using y_list with values from [-1,1]\n", "print(\"tanh loss (w1=3, w2=1, b=-2) [scale = 1]\")\n", "print(tanh_loss(x_list,y_list,[3.,1.],-2.))"]}, {"cell_type": "markdown", "id": "2bae4aa4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_13_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L13.5 What Do Neural Networks Do?</h2>     \n", "\n", "| [Top](#section_13_0) | [Previous Section](#section_13_4) | [Exercises](#exercises_13_5) | [Next Section](#section_13_6) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "ad79573d", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.5-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L13/slides_L13_05.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "824b6631", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_13_5'></a>   \n", "\n", "| [Top](#section_13_0) | [Restart Section](#section_13_5) | [Next Section](#section_13_6) |"]}, {"cell_type": "markdown", "id": "bd65e1ad", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.5.1</span>\n", "\n", "Which of the following statements accurately describes how backpropagation works in neural networks?\n", "\n", "A) Backpropagation calculates the forward pass of the neural network to generate predictions, and then updates the weights and biases based on the difference between the predicted and true values.\n", "\n", "B) Backpropagation computes the gradient of the loss function with respect to the weights and biases of the neural network, and uses this gradient to update the parameters of the model.\n", "\n", "C) Backpropagation calculates the error of each individual neuron in the neural network, and adjusts the weights and biases of each neuron based on its individual error.\n", "\n", "D) Backpropagation randomly samples a subset of the training data and updates the weights and biases of the neural network based on the error on this subset, repeating the process until convergence is reached."]}, {"cell_type": "markdown", "id": "05bc938a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.5.2</span>\n", "\n", "Let's return to the alternate activation function that we previously considered: the `tanh` function. Which of the following statements accurately describes the usefulness of using the `tanh` activation function, compared to other functions, for an output node in a binary classification task? Consider what we have just learned about the training process.\n", "\n", "Select all that apply:\n", "\n", "A) The tanh activation function is a symmetric function that captures both positive and negative patterns equally well.\\\n", "B) The gradients of the tanh function are relatively steeper compared to the sigmoid function, especially in the region closer to the origin. This can lead to faster convergence during training.\\\n", "C) When the input values to the tanh function are large, the gradients become close to zero, leading to faster convergence.\\\n", "D) None of the above."]}, {"cell_type": "markdown", "id": "fbbafcac", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_13_6'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L13.6 Training a Neural Network</h2>  \n", "\n", "\n", "| [Top](#section_13_0) | [Previous Section](#section_13_5) | [Exercises](#exercises_13_6) | [Next Section](#section_13_7) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "498121e5", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 564}, "id": "-oK2xgwHxVsy", "outputId": "64c83ae6-1abb-43e5-f112-3be23f91efba", "tags": ["learner", "py", "lect_06", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.6-runcell01\n", "\n", "#the elements of the array x0 are defined as the following:\n", "#x0[0] -> w[0]\n", "#x0[1] -> w[1]\n", "#x0[2] -> b\n", "#x0[3] -> scale\n", "x0 = np.array([0,0,0,1.])\n", "\n", "\n", "def fun(inputs):    \n", "  #recall the inputs of lr_loss(): lr_loss(x,y,w,b,scale=1.)\n", "  loss=lr_loss(x_list,y_list_0,[inputs[0],inputs[1]],inputs[2],inputs[3])\n", "  return loss\n", "\n", "sol=opt.minimize(fun,x0)\n", "print(sol)\n", "print()\n", "\n", "#we print the weights and b,\n", "#rescaled such that they correspond to our original definition of the boundary\n", "print(\"(w1, w2, b):\", sol.x[3]*sol.x[0], sol.x[3]*sol.x[1], sol.x[3]*sol.x[2])\n", "\n", "\n", "#Now plot it\n", "h_grid = 1./(1.+np.exp(-1.*sol.x[3]*(sol.x[0]*x1_grid+sol.x[1]*x2_grid + sol.x[2] )))\n", "\n", "plt.pcolormesh(x1_grid, x2_grid, h_grid, cmap = 'bwr', shading='auto')\n", "plt.colorbar()\n", "\n", "plt.plot(x1_pos, x2_pos, 'k+')\n", "plt.plot(x1_neg, x2_neg, 'k.')\n", "\n", "plt.show()"]}, {"cell_type": "markdown", "id": "34c68dfd", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_13_6'></a>     \n", "\n", "| [Top](#section_13_0) | [Restart Section](#section_13_6) | [Next Section](#section_13_7) |\n"]}, {"cell_type": "markdown", "id": "ed755350", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.6.1</span>\n", "\n", "What happens to the best fit parameters of the logistic regression model when you force the scale to be 1? Try this yourself in the code cell provided!\n", "\n", "Choose the best answer from the options below:\n", "\n", "A) Rescaled parameters are exactly the same, to within many decimal places.\\\n", "B) Rescaled parameters change a little, but not significantly.\\\n", "C) The rescaled parameters all decrease.\\\n", "D) The rescaled parameters all increase."]}, {"cell_type": "code", "execution_count": null, "id": "6ea5fcc9", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L13.6.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n"]}, {"cell_type": "markdown", "id": "b5bef474", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_13_7'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L13.7 Training a Neural Network with PyTorch</h2>  \n", "\n", "\n", "| [Top](#section_13_0) | [Previous Section](#section_13_6) | [Exercises](#exercises_13_7) |"]}, {"cell_type": "code", "execution_count": null, "id": "15c47628", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Zc_UW4dSOCJ0", "outputId": "32dc5f8d-a657-4a09-ac77-a5662df9f002", "tags": ["learner", "py", "lect_07", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.7-runcell01\n", "\n", "#now let's do this with a neural network\n", "\n", "torch.random.manual_seed(42) # Set the random seed\n", "\n", "class simple_MLP(torch.nn.Module):\n", "    def __init__(self):\n", "        super().__init__()\n", "        self.fc1 = torch.nn.Linear(2,1)\n", "        self.output = torch.nn.Sigmoid()\n", "\n", "    def forward(self, x):\n", "        x = self.fc1(x)\n", "        x = self.output(x)\n", "        return x\n", "    \n", "\n", "simple_model = simple_MLP()\n", "print(simple_model.named_parameters('fcl'))\n", "print('----------')\n", "print(simple_model.state_dict())"]}, {"cell_type": "code", "execution_count": null, "id": "f9ad0b0e", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 495}, "id": "ZinJyWitO_PL", "outputId": "3547379d-2666-4005-cd36-bc6bba74ed8f", "scrolled": true, "tags": ["learner", "py", "lect_07", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.7-runcell02\n", "\n", "\n", "#------------------------\n", "#Define lists and grids (this was done previously in this notebook)\n", "#create grid\n", "x1_list = np.linspace(-20., 20., 100)\n", "x2_list = np.linspace(-20., 20., 100)\n", "x1_grid, x2_grid  = np.meshgrid(x1_list, x2_list)\n", "#------------------------\n", "\n", "\n", "#Create a loss and minimizer \n", "simple_criterion = torch.nn.BCELoss() #BCE => Binary Cross Entropy\n", "simple_optimizer = torch.optim.Adam(simple_model.parameters(), lr=0.01) \n", "simple_history = {'loss':[]}\n", "\n", "#pytorch\n", "x_list_np = np.transpose(np.vstack((x_list[0],x_list[1])))\n", "y_list_np = (np.array(y_list)+1)/2\n", "x_torch=torch.from_numpy(x_list_np).float()\n", "y_torch=torch.from_numpy(y_list_np.reshape(100,1)).float()\n", "\n", "for epoch in range(100):\n", "    simple_optimizer.zero_grad()\n", "    outputs = simple_model(x_torch)\n", "    loss = simple_criterion(outputs, y_torch)\n", "    loss.backward()\n", "    simple_optimizer.step()    \n", "    # add loss statistics\n", "    current_loss = loss.item()\n", "    print('[%d] loss: %.5f  ' % (epoch + 1,  current_loss))\n", "    simple_history['loss'].append(current_loss)\n", "            \n", "print('Finished Training')\n", "torch.save(simple_model.state_dict(), 'data/L13/simple_model.pt')\n", "print(simple_model.state_dict())"]}, {"cell_type": "code", "execution_count": null, "id": "d545c7c0", "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 495}, "id": "ZinJyWitO_PL", "outputId": "3547379d-2666-4005-cd36-bc6bba74ed8f", "scrolled": false, "tags": ["learner", "py", "lect_07", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L13.7-runcell03\n", "\n", "#Now plot it\n", "x_grid_np = np.transpose(np.vstack((x1_grid.flatten(),x2_grid.flatten())))\n", "x_torch=torch.from_numpy(x_grid_np).float()\n", "h_grid = simple_model(x_torch)\n", "h_grid=h_grid.detach().numpy()\n", "h_grid=h_grid.reshape(100,100)\n", "print(h_grid.shape)\n", "plt.pcolormesh(x1_grid, x2_grid, h_grid, cmap = 'bwr', shading='auto')\n", "plt.colorbar()\n", "\n", "plt.plot(x1_pos, x2_pos, 'k+')\n", "plt.plot(x1_neg, x2_neg, 'k.')\n", "\n", "plt.show()\n", "\n", "print(simple_model.state_dict())"]}, {"cell_type": "markdown", "id": "65162d2b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='exercises_13_7'></a>     \n", "\n", "| [Top](#section_13_0) | [Restart Section](#section_13_7) |"]}, {"cell_type": "markdown", "id": "8552a25d", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.7.1</span>\n", "\n", "Run code cell `L13.7-runcell02` until the optimizer reaches a minimum loss value which no longer changes. There are two ways to do this. One option is to simply rerun `L13.7-runcell02` over and over, since it will start each time with the final parameters found in the previous run. Another option is to first run code cell `L13.7-runcell01` to reinitialize the parameters, and then increase the number of epochs when running `L13.7-runcell02`. See the solution for a more detailed discussion of these two options. Rerunning `L13.7-runcell02` is a fine approach.\n", "\n", "When the loss is minimized, what are the best fit slope parameters of the neural network? How do they compare to the output of our minimizer in the previous section? Print the values from `simple_model.fc1.weight`, using the code cell provided.\n", "\n", "Enter your answer as a list of two numbers with precision 1e-2: `[w1, w2]`"]}, {"cell_type": "code", "execution_count": null, "id": "62081ce6", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L13.7.1\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "print(\"w1:\", #YOUR CODE HERE)\n", "print(\"w2:\", #YOUR CODE HERE)"]}, {"cell_type": "markdown", "id": "828c6cb8", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.7.2</span>\n", "\n", "Vary the learning rate, which is the parameter `lr` in the function `torch.optim.Adam(simple_model.parameters(), lr=0.01)`. In order to see what happens when you don't start with the previously found best-fit (or in some cases very bad fit) values of the parameters, run code `cell L13.7-runcell01` to first reinitialize the parameters. Do this each time you change `lr` in code cell `L13.7-runcell02`. \n", "\n", "What happens when the learning rate is very large (e.g., approaching 1)?\n", "\n", "\n", "A) The network does not converge to a solution.\\\n", "B) The network converges more quickly.\\\n", "C) The learning rate does not affect the rate of convergence.\n", "\n"]}, {"cell_type": "markdown", "id": "47606638", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-13.7.3</span>\n", "\n", "Try changing the number of epochs and the number of times that you run the cell to see what effect this has on how quickly the network minimizes the loss. When trying different number of epochs, re-initialize the model by first running code cell `L13.7-runcell01`.\n", "\n", "What did you discover after varying the different ways that the neural network is run? Choose the best option below:\n", "\n", "A) The network always minimizes the loss on the first run.\\\n", "B) The network is not sensitive to the way in which it is run.\\\n", "C) The network will always find a minimum if the epoch is long enough.\\\n", "D) The network will always find a minimum if it is reinitialized.\\\n", "E) None of the above."]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}