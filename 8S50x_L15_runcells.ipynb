{"cells": [{"cell_type": "markdown", "id": "1ee373c5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<hr style=\"height: 1px;\">\n", "<i>This notebook was authored by the 8.S50x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n", "<hr style=\"height: 1px;\">\n", "<br>\n", "\n", "<h1>Lesson 15: Deep Learning Regression</h1>\n"]}, {"cell_type": "markdown", "id": "41345f3a", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_0'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.0 Overview</h2>\n"]}, {"cell_type": "markdown", "id": "fce9b388", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<h3>Navigation</h3>\n", "\n", "<table style=\"width:100%\">\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_15_1\">L15.1 Discovering the Higgs with Deep Learning</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_15_1\">L15.1 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_15_2\">L15.2 Minimizing Loss with a Neural Network</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_15_2\">L15.2 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_15_3\">L15.3 An Example with PyTorch: Fitting a Parabola</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_15_3\">L15.3 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_15_4\">L15.4 Another Example: Fitting a Sine Function</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_15_4\">L15.4 Exercises</a></td>\n", "    </tr>\n", "    <tr>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_15_5\">L15.5 Sine Function Continued: Adjusting the Network</a></td>\n", "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_15_5\">L15.5 Exercises</a></td>\n", "    </tr>\n", "</table>"]}, {"cell_type": "code", "execution_count": null, "id": "d59f3ad0", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.0-runcell00\n", "\n", "!pip install torch\n", "!pip install imageio\n", "!pip install awkward\n", "!pip install george\n", "!pip install uproot\n", "!pip install pylorentz"]}, {"cell_type": "code", "execution_count": null, "id": "0f3a0154", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.0-runcell01\n", "\n", "import torch                        #https://pytorch.org/docs/stable/torch.html\n", "import torch.nn as nn               #https://pytorch.org/docs/stable/nn.html\n", "from torch.autograd import Variable #https://pytorch.org/docs/stable/autograd.html\n", "import torch.nn.functional as F     #https://pytorch.org/docs/stable/nn.functional.html\n", "import torch.utils.data as Data     #https://pytorch.org/docs/stable/data.html\n", "\n", "import matplotlib.pyplot as plt     #https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html\n", "%matplotlib inline\n", "\n", "import numpy as np                  #https://numpy.org/doc/stable/\n", "import imageio                      #https://imageio.readthedocs.io/en/stable/\n", "import george                       #https://george.readthedocs.io/en/latest/\n", "from george import kernels          #https://george.readthedocs.io/en/latest/user/kernels/"]}, {"cell_type": "code", "execution_count": null, "id": "97d0edf8", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.0-runcell02\n", "\n", "#set plot resolution\n", "%config InlineBackend.figure_format = 'retina'\n", "\n", "#set default figure parameters\n", "plt.rcParams['figure.figsize'] = (9,6)\n", "\n", "medium_size = 12\n", "large_size = 15\n", "\n", "plt.rc('font', size=medium_size)          # default text sizes\n", "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n", "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n", "plt.rc('legend', fontsize=medium_size)    # legend\n", "plt.rc('axes', titlesize=large_size)      # axes title\n", "plt.rc('axes', labelsize=large_size)      # x and y labels\n", "plt.rc('figure', titlesize=large_size)    # figure title"]}, {"cell_type": "markdown", "id": "1f186cb0", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_1'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.1 Discovering the Higgs with Deep Learning</h2>  \n", "\n", "| [Top](#section_15_0) | [Previous Section](#section_15_0) | [Exercises](#exercises_15_1) | [Next Section](#section_15_2) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "9d3798ed", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.1-slides\n", "\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L19/slides_L19_01.html', width=970, height=550)"]}, {"cell_type": "markdown", "id": "eb71daaa", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.1.1</span>\n", "\n", "Look again at the Higgs discovery plots. At the Higgs mass, the CMS experiment is about 30% more sensitive than ATLAS, but both experiments found approximately the same excess. This is because:\n", "\n", "A) The ATLAS experiment was less sensitive, but had more data.\\\n", "B) The ATLAS experiment had more advanced machine learning analysis tools.\\\n", "C) This was just a random fluctuation, and not a particularly unlikely one.\\\n", "D) The CMS data was noisier."]}, {"cell_type": "markdown", "id": "e446de2e", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_2'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.2 Minimizing Loss with a Neural Network</h2>  \n", "\n", "| [Top](#section_15_0) | [Previous Section](#section_15_1) | [Exercises](#exercises_15_2) | [Next Section](#section_15_3) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "9fef113b", "metadata": {"tags": ["learner", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.2-slides\n", "\n", "from IPython.display import IFrame\n", "IFrame(src='https://mitx-8s50.github.io/slides/L19/slides_L19_02.html', width=970, height=550)"]}, {"cell_type": "code", "execution_count": null, "id": "83bf6d2b", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.2-runcell01\n", "\n", "#Let's Try GP on a Gaussian random set of points\n", "def gaussian(mu,sigma,norm,offset):\n", "    \"\"\"Returns a gaussian function with the given parameters\"\"\"\n", "    return lambda x: norm*np.exp(-(1./2.)*((x-mu)/(sigma))**2)+offset\n", "\n", "Xin = np.mgrid[0:201] # points from 0 to 201\n", "data = gaussian(100., 20., 10., 5)(Xin) + 5*np.random.random(Xin.shape) # Guassian + semaring\n", "plt.plot(data,\"*\")\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "9f394406", "metadata": {"tags": ["learner", "py", "lect_02", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.2-runcell02\n", "\n", "#now let's run GP on this guy\n", "\n", "kernel = np.var(data) * kernels.ExpSquaredKernel(1.5)\n", "gp = george.GP(kernel)\n", "var=np.ones(len(Xin))\n", "gp.compute(Xin,var)\n", "x_pred = np.linspace(0, 200, 100)\n", "pred, pred_var = gp.predict(data, x_pred, return_var=True)\n", "\n", "plt.fill_between(x_pred, pred - np.sqrt(pred_var), pred + np.sqrt(pred_var),color=\"k\", alpha=0.2)\n", "plt.plot(x_pred, pred, \"k\", lw=1.5, alpha=0.5)\n", "plt.plot(Xin,data,\"*\")\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"y\");"]}, {"cell_type": "markdown", "id": "9ec4af2f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.2.1</span>\n", "\n", "In the last lesson, when we applied a neural network to separate two samples, we cared only about minimizing the binary cross entropy. We did not care about knowing where the points are distributed. When we fit a function, we need to guess a form to fit the data and minimize $\\chi^2$. Do we need to do that for a neural network?  \n", "\n", "A) The neural network needs an architecture that is similar to the functional guess.\\\n", "B) If there are sufficient parameters, the neural network can approximate the function, no matter the form.\\\n", "C) Specific architectures are needed for specific problems (CNNs for images, RNNs for time series).\\\n", "D) The neural network needs to work with our statistical tools like Gaussian processes and f-tests to get the right function."]}, {"cell_type": "markdown", "id": "02b3a423", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.2.2</span>\n", "\n", "In the preceding code cell, `L15.2-runcell02`, we used the Gaussian Process regression algorithm to fit a curve to some data. Explore using different metrics in the function <a href=\"https://dfm.io/george/dev/user/kernels/#george.kernels.ExpSquaredKernel\" target=\"_blank\">`george.kernels.ExpSquaredKernel`</a> (currently, the default is set to 1.5). The code cell is repeated below, in the notebook. \n", "\n", "What approximate value of the metric should one use to smooth out the extraneous wiggles in the plot?\n", "\n", "A) 0.5\\\n", "B) 2.5\\\n", "C) 5\\\n", "D) 20\\\n", "E) 100"]}, {"cell_type": "code", "execution_count": null, "id": "4d698bca", "metadata": {"tags": ["py", "learner_chopped", "draft"]}, "outputs": [], "source": ["#>>>EXERCISE: L15.2.2\n", "\n", "#now let's run GP on this guy\n", "\n", "kernel = np.var(data) * kernels.ExpSquaredKernel(1.5)\n", "gp = george.GP(kernel)\n", "var=np.ones(len(Xin))\n", "gp.compute(Xin,var)\n", "x_pred = np.linspace(0, 200, 100)\n", "pred, pred_var = gp.predict(data, x_pred, return_var=True)\n", "\n", "plt.fill_between(x_pred, pred - np.sqrt(pred_var), pred + np.sqrt(pred_var),color=\"k\", alpha=0.2)\n", "plt.plot(x_pred, pred, \"k\", lw=1.5, alpha=0.5)\n", "plt.plot(Xin,data,\"*\")\n", "plt.xlabel(\"x\")\n", "plt.ylabel(\"y\");"]}, {"cell_type": "markdown", "id": "1927751b", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_3'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.3 An Example with PyTorch: Fitting a Parabola</h2>  \n", "\n", "| [Top](#section_15_0) | [Previous Section](#section_15_2) | [Exercises](#exercises_15_3) | [Next Section](#section_15_4) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "f60bceb6", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.3-runcell01\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "x = torch.unsqueeze(torch.linspace(-1, 1, 100), dim=1)  # x data (tensor), shape=(100, 1)\n", "y = x.pow(2) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n", "\n", "# torch can only train on Variable, so convert them to Variable\n", "x, y = Variable(x), Variable(y)\n", "\n", "# view data\n", "plt.figure(figsize=(10,4))\n", "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "plt.title('Regression Analysis')\n", "plt.xlabel('Independent variable')\n", "plt.ylabel('Dependent variable')\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "ac9b50bd", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.3-runcell02\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "# this is one way to define a network\n", "class Net(torch.nn.Module):\n", "    def __init__(self, n_feature, n_hidden, n_output):\n", "        super(Net, self).__init__()\n", "        self.hidden = torch.nn.Linear(n_feature, n_hidden)   # hidden layer\n", "        self.predict = torch.nn.Linear(n_hidden, n_output)   # output layer\n", "\n", "    def forward(self, x):\n", "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n", "        x = self.predict(x)             # linear output\n", "        return x\n", "\n", "net = Net(n_feature=1, n_hidden=10, n_output=1)     # define the network\n", "print(net)  # net architecture\n", "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)#stochastic Gradient Descent\n", "loss_func = torch.nn.MSELoss()  # this is for regression mean squared loss"]}, {"cell_type": "code", "execution_count": null, "id": "29052969", "metadata": {"tags": ["learner", "py", "lect_03", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.3-runcell03\n", "\n", "def makePlot(x,y,prediction,ax,fig,images,t,loss,ymin,ymax):\n", "    # plot and show learning process\n", "    plt.cla()\n", "    ax.set_title('Regression Analysis', fontsize=35)\n", "    ax.set_xlabel('Independent variable', fontsize=24)\n", "    ax.set_ylabel('Dependent variable', fontsize=24)\n", "    ax.set_ylim(ymin,ymax)\n", "    ax.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "    ax.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n", "    ax.text(0.6, 0.7, 'Epoch = %d' % t, fontdict={'size': 24, 'color':  'red'})\n", "    ax.text(0.6, 0.3, 'Loss = %.4f' % loss.data.numpy(),fontdict={'size': 24, 'color':  'red'}) \n", "    fig.canvas.draw()       # draw the canvas, cache the renderer\n", "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n", "    image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n", "    images.append(image)\n", "\n", "def train(x,y,net,loss_func,opt,nepochs,ymin,ymax):\n", "    images = []\n", "    fig, ax = plt.subplots(figsize=(12,7))\n", "    for epoch in range(nepochs):\n", "        if epoch % 50 == 0: \n", "            print(\"epoch:\",epoch)\n", "        prediction = net(x)\n", "        loss = loss_func(prediction, y) \n", "        opt.zero_grad()\n", "        loss.backward() \n", "        optimizer.step()\n", "        nplots = int(nepochs/40)\n", "        if epoch % nplots == 0:\n", "            makePlot(x,y,prediction,ax,fig,images,epoch,loss,ymin,ymax)\n", "    return images\n", "    \n", "from IPython.display import Image\n", "images=train(x,y,net,loss_func,optimizer,200,-0.1,1.5)\n", "imageio.mimsave('data/L15/curve_1.gif', images, fps=10)\n", "Image(open('data/L15/curve_1.gif','rb').read())"]}, {"cell_type": "markdown", "id": "64d0bc45", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.3.1</span>\n", "\n", "What are the dimensions of the input, output, and hidden layers? Enter your answer as a list of integers: `[input, output, hidden]`"]}, {"cell_type": "markdown", "id": "7d88eff8", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.3.2</span>\n", "\n", "Why do we choose to use mean squared error for our loss function? Select all the apply:\n", "\n", "A) This is the same as our original fit minimization (least squares).\\\n", "B) We did not give uncertainties, so we cannot do chi2.\\\n", "C) Mean squared error is the closest we get to absolute value.\\\n", "D) Actually, we should not use a NN at all, and we really should do a full systematic approach of adding polynomials and fitting to determine the functional form."]}, {"cell_type": "markdown", "id": "f630da0f", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.3.3</span>\n", "\n", "You saw that increasing the number of hidden parameters (i.e. the number of neurons in the hidden layer) gave a better by-eye fit. Why not use as many hidden parameters as possible? Wouldn't this make an even better fit to the data? Why would it NOT be ideal to arbitrarily add more hidden parameters? Select all that apply:\n", "\n", "A) It can lead to overfitting of the model to the training data.\\\n", "B) It can make the model too simple and unable to learn complex features.\\\n", "C) It can require more computational resources.\\\n", "D) It can decrease the accuracy of the model on the training data.\\\n", "E) There is an ideal number of hidden parameters that works universally well for all neural networks.\n"]}, {"cell_type": "markdown", "id": "be2f50d3", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_4'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.4 Another Example: Fitting a Sine Function</h2>  \n", "\n", "| [Top](#section_15_0) | [Previous Section](#section_15_3) | [Exercises](#exercises_15_4) | [Next Section](#section_15_5) |\n"]}, {"cell_type": "code", "execution_count": null, "id": "c6069574", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.4-runcell01\n", "\n", "torch.manual_seed(1)    # reproducible\n", "x = torch.unsqueeze(torch.linspace(-10, 10, 1000), dim=1)  # x data (tensor), shape=(100, 1)\n", "y = torch.sin(x) + 0.2*torch.rand(x.size())                 # noisy y data (tensor), shape=(100, 1)\n", "\n", "# torch can only train on Variable, so convert them to Variable\n", "x, y = Variable(x), Variable(y)\n", "plt.figure(figsize=(10,4))\n", "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "plt.title('Regression Analysis')\n", "plt.xlabel('Independent variable')\n", "plt.ylabel('Dependent variable')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "d0ceaf00", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.4-runcell02\n", "\n", "#redefine network\n", "net = Net(n_feature=1, n_hidden=10, n_output=1)     # define the network, try changing to n_hidden=100\n", "#net = Net(n_feature=1, n_hidden=100, n_output=1)\n", "\n", "images=train(x,y,net,loss_func,optimizer,200,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "code", "execution_count": null, "id": "298a29ea", "metadata": {"tags": ["learner", "py", "lect_04", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.4-runcell03\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "# another way to define a network\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.Linear(100, 100),\n", "        torch.nn.Linear(100, 1),\n", "    )\n", "print(net[0].weight[0:10])\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,200,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "markdown", "id": "086d2035", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.4.1</span>\n", "\n", "Which of the following statements describes why linear activation function layers are not suitable for classification of data which is strongly nonlinear?\n", "\n", "A) A linear activation function cannot capture complex nonlinear relationships between the input features and the output classes.\\\n", "B) A linear activation function is only effective for classification tasks with linearly separable data.\\\n", "C) A linear activation function is prone to underfitting and can lead to poor performance on the training and test sets.\\\n", "D) A linear activation function can introduce too much noise into the model and reduce its accuracy."]}, {"cell_type": "markdown", "id": "c3c9cac5", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["<a name='section_15_5'></a>\n", "<hr style=\"height: 1px;\">\n", "\n", "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L15.5 Sine Function Continued: Adjusting the Network</h2>  \n", "\n", "| [Top](#section_15_0) | [Previous Section](#section_15_4) | [Exercises](#exercises_15_5) |"]}, {"cell_type": "code", "execution_count": null, "id": "437687d2", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell01\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "# another way to define a network\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,200,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "code", "execution_count": null, "id": "76a868ed", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell02\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "# another way to define a network\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,400,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "code", "execution_count": null, "id": "3002338f", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell03\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,400,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "code", "execution_count": null, "id": "6fd24f06", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell04\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 100),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(100, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "loss_func = torch.nn.MSELoss()\n", "images=train(x,y,net,loss_func,optimizer,400,-1.1,1.1)\n", "imageio.mimsave('data/L15/curve_2.gif', images, fps=10)\n", "Image(open('data/L15/curve_2.gif','rb').read())"]}, {"cell_type": "code", "execution_count": null, "id": "9da1e90e", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell05\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "x=torch.unsqueeze(torch.linspace(0, 200, 201), dim=1) \n", "y=torch.from_numpy(data.reshape(len(data),1).astype('float32'))\n", "\n", "# torch can only train on Variable, so convert them to Variable\n", "x, y = Variable(x), Variable(y)\n", "\n", "# view data\n", "plt.figure(figsize=(10,4))\n", "plt.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "plt.title('Regression Analysis')\n", "plt.xlabel('Independent variable')\n", "plt.ylabel('Dependent variable')\n", "plt.show()\n"]}, {"cell_type": "code", "execution_count": null, "id": "7186c0e6", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell06\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "def makePlot(x,y,prediction,ax,fig,images,t,loss,ymin,ymax):\n", "    # plot and show learning process\n", "    plt.cla()\n", "    ax.set_title('Regression Analysis', fontsize=35)\n", "    ax.set_xlabel('Independent variable', fontsize=24)\n", "    ax.set_ylabel('Dependent variable', fontsize=24)\n", "    ax.set_ylim(ymin,ymax)\n", "    ax.scatter(x.data.numpy(), y.data.numpy(), color = \"orange\")\n", "    ax.plot(x.data.numpy(), prediction.data.numpy(), 'g-', lw=3)\n", "    ax.text(125, 16, 'Epoch = %d' % t, fontdict={'size': 24, 'color':  'red'})\n", "    ax.text(125, 14, 'Loss = %.4f' % loss.data.numpy(),fontdict={'size': 24, 'color':  'red'}) \n", "    fig.canvas.draw()       # draw the canvas, cache the renderer\n", "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n", "    image  = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n", "    images.append(image)\n", "\n", "\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 10),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(10, 10),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(10, 10),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(10, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "images=train(x,y,net,loss_func,optimizer,300,5,20)\n", "imageio.mimsave('data/L15/curve_gaus.gif', images, fps=10)\n", "Image(open('data/L15/curve_gaus.gif','rb').read())"]}, {"cell_type": "code", "execution_count": null, "id": "5b2a149b", "metadata": {"tags": ["learner", "py", "lect_05", "learner_chopped"]}, "outputs": [], "source": ["#>>>RUN: L15.5-runcell07\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "n_hidden=10\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "images=train(x,y,net,loss_func,optimizer,1000,5,20)\n", "imageio.mimsave('data/L15/curve_gaus.gif', images, fps=10)\n", "Image(open('data/L15/curve_gaus.gif','rb').read())"]}, {"cell_type": "markdown", "id": "e570f505", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.5.1</span>\n", "\n", "Consider the following neural network. How many hidden layers are there in this model? Enter your answer as an integer.\n", "\n", "<pre>\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 50),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(50, 1),\n", "    )\n", "</pre>\n"]}, {"cell_type": "markdown", "id": "d64bf7a4", "metadata": {"tags": ["learner", "md", "learner_chopped"]}, "source": ["### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 15.5.2</span>\n", "\n", "Run the code cell shown below multiple times, **varying the number of hidden layers each time.** Which of the following statements describes the effect of increasing the number of hidden layers? Select all that apply:\n", "\n", "A) Increasing the number of hidden layers always leads to better classification performance.\\\n", "B) Increasing the number of hidden layers can help the model learn more complex patterns and improve classification performance, but may also increase the risk of overfitting.\\\n", "C) Increasing the number of hidden layers increases computational load and should only be done if better performance is needed.\\\n", "D) Increasing the number of hidden layers is only beneficial if the number of neurons in each layer is also increased.\n", "\n", "**Extra:** Try also varying the number of parameters within the hidden layers."]}, {"cell_type": "code", "execution_count": null, "id": "f6a7bb49", "metadata": {"tags": ["draft", "py", "learner_chopped"]}, "outputs": [], "source": ["#>>>EXERCISE: L15.5.2\n", "# Use this cell for drafting your solution (if desired),\n", "# then enter your solution in the interactive problem online to be graded.\n", "\n", "def train(x,y,net,loss_func,opt,nepochs,ymin,ymax):\n", "    images = []\n", "    fig, ax = plt.subplots(figsize=(12,7))\n", "    for epoch in range(nepochs):\n", "        if epoch % 200 == 0: \n", "            print(\"epoch:\",epoch)\n", "        prediction = net(x)\n", "        loss = loss_func(prediction, y) \n", "        opt.zero_grad()\n", "        loss.backward() \n", "        optimizer.step()\n", "        # Minimize plots for faster running\n", "        if epoch == nepochs-1:\n", "            makePlot(x,y,prediction,ax,fig,images,epoch,loss,ymin,ymax)\n", "    return images\n", "\n", "torch.manual_seed(1)    # reproducible\n", "\n", "n_hidden=30\n", "\n", "#VARY THE NUMBER OF HIDDEN LAYERS BELOW\n", "net = torch.nn.Sequential(\n", "        torch.nn.Linear(1, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, n_hidden),\n", "        torch.nn.ReLU(),\n", "        torch.nn.Linear(n_hidden, 1),\n", "    )\n", "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n", "images=train(x,y,net,loss_func,optimizer,1000,5,20)\n", "imageio.mimsave('data/L15/curve_gaus.gif', images, fps=10)\n", "Image(open('data/L15/curve_gaus.gif','rb').read())\n"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.9"}}, "nbformat": 4, "nbformat_minor": 5}